{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet101V2\n",
    "\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "\n",
    "base_path = \"C:/Users/psaik/Documents/CIP/\"\n",
    "lables = [\"edible\", \"poisonous\"]\n",
    "\n",
    "# Directories containing images for each category\n",
    "directory_group = [\n",
    "    ['edible mushroom sporocarp', 'edible sporocarp'], \n",
    "    ['poisonous mushroom sporocarp', 'poisonous sporocarp']\n",
    "]\n",
    "\n",
    "# Lists to store file paths for each category\n",
    "edible_fungus = []\n",
    "poisonous_fungus = []\n",
    "\n",
    "# Loop through each directory and add file paths to corresponding category list\n",
    "for (label, directories) in zip(lables, directory_group):\n",
    "    for directory in directories:\n",
    "        items = os.listdir(base_path + directory)\n",
    "        for item in items:\n",
    "            file_path = base_path + directory + \"/\" + item\n",
    "            if label == \"edible\":\n",
    "                edible_fungus.append(file_path)\n",
    "            else:\n",
    "                poisonous_fungus.append(file_path)\n",
    "\n",
    "# Remove duplicate file paths and convert to lists\n",
    "edible_fungus = list(set(edible_fungus))\n",
    "poisonous_fungus = list(set(poisonous_fungus))\n",
    "\n",
    "# Define batch size and validation split\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "\n",
    "# Split data into training and validation sets\n",
    "edible_fungus_split_index = int((1 - validation_split) * len(edible_fungus))\n",
    "poisonous_fungus_split_index = int((1 - validation_split) * len(poisonous_fungus))\n",
    "train_edible_fungus, valid_edible_fungus = edible_fungus[:edible_fungus_split_index],  edible_fungus[edible_fungus_split_index:] \n",
    "train_poisonous_fungus, valid_poisonous_fungus = poisonous_fungus[:poisonous_fungus_split_index],  poisonous_fungus[poisonous_fungus_split_index:] \n",
    "\n",
    "# Print number of training and validation samples for each category\n",
    "print(len(train_edible_fungus), len(valid_edible_fungus))\n",
    "print(len(train_poisonous_fungus), len(valid_poisonous_fungus))\n",
    "\n",
    "# Calculate number of batches per epoch based on batch size\n",
    "num_batch_per_epoch = min(len(train_edible_fungus), len(train_poisonous_fungus)) // batch_size\n",
    "print(num_batch_per_epoch)\n",
    "\n",
    "# Set number of epochs and convert data to numpy arrays\n",
    "num_epochs = 50\n",
    "train_edible_fungus = np.array(train_edible_fungus)\n",
    "valid_edible_fungus = np.array(valid_edible_fungus)\n",
    "train_poisonous_fungus = np.array(train_poisonous_fungus)\n",
    "valid_poisonous_fungus = np.array(valid_poisonous_fungus)\n",
    "\n",
    "# Calculate total number of validation samples\n",
    "total_valid_count = len(valid_edible_fungus) + len(valid_poisonous_fungus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing function that reads and resizes images\n",
    "def preprocess_image(item):\n",
    "    # Read image file as a string\n",
    "    image_string = tf.io.read_file(item[0])\n",
    "    \n",
    "    # Decode the image based on the file type using a conditional statement\n",
    "    image_decoded = tf.cond(\n",
    "        tf.image.is_jpeg(image_string),  # Check if the image is a JPEG\n",
    "        lambda: tf.image.decode_jpeg(image_string, channels=3),  # Decode as JPEG, JPG\n",
    "        lambda: tf.image.decode_png(image_string, channels=3)    # Decode as PNG\n",
    "    )\n",
    "    \n",
    "    # Resize the decoded image\n",
    "    image_resized = tf.image.resize(image_decoded, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    \n",
    "    # Normalize the image pixels to be in the range [0, 1]\n",
    "    image_normalized = tf.cast(image_resized, tf.float32) / 255.0\n",
    "    \n",
    "    # Convert the label to an integer\n",
    "    label = tf.strings.to_number(item[1], tf.int64)\n",
    "    \n",
    "    return image_normalized, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load and display an image from a file path\n",
    "from PIL import Image\n",
    "def show_image(file_path, category):\n",
    "    img = tf.keras.preprocessing.image.load_img(file_path, target_size=(240, 240)) # Load image and resize to 240x240 pixels\n",
    "    plt.imshow(img) # Display the image\n",
    "    plt.title(category)\n",
    "    plt.show() # Show the plot\n",
    "\n",
    "import random\n",
    "random.seed(42) # Set a random seed for reproducibility\n",
    "\n",
    "random.shuffle(edible_fungus) # Shuffle the list of edible fungus image file paths randomly\n",
    "random.shuffle(poisonous_fungus)\n",
    "\n",
    "show_image(edible_fungus[0], 'edible') \n",
    "show_image(poisonous_fungus[0], 'poisonous')\n",
    "\n",
    "# from PIL import Image\n",
    "# def show_image(file_path, category):\n",
    "#     img = Image.open(file_path) # Load image using Pillow\n",
    "#     img = img.resize((240, 240)) # Resize to 240x240 pixels\n",
    "#     img.show() # External/Default image viewer\n",
    "#     print(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Tensorflow dataset for training and evaluation\n",
    "# mode - train or eval, edible_fungus & poisonous_fungus - list of file paths\n",
    "def get_dataset(edible_fungus, poisonous_fungus, mode, batch_size):\n",
    "    # Combine into a single list\n",
    "    x = list(edible_fungus) + list(poisonous_fungus)\n",
    "    \n",
    "    # Create a binary label for each mushroom indicating whether it is poisonous or not\n",
    "    y = [0] * len(edible_fungus) + [1] * len(poisonous_fungus)\n",
    "    \n",
    "    # Combine the features and labels into a list of tuples\n",
    "    items = [(a, b) for (a, b) in zip(x, y)]\n",
    "    \n",
    "    # Create a TensorFlow dataset from the list of tuples, shuffle the items randomly, and batch the data\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(np.array(items)).shuffle(len(x))\n",
    "    dataset = dataset.map(preprocess_image).batch(batch_size)\n",
    "    \n",
    "    # Return the dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a balanced dataset\n",
    "def get_balanced_dataset(edible_fungus, poisonous_fungus, batch_count, batch_size, mode=\"train\"):\n",
    "    # Calc the number of samples per category\n",
    "    length_per_category = batch_size * batch_count // 2\n",
    "    # Randomly select indices of edible and poisonous fungi images\n",
    "    edible_indices = np.random.choice(len(edible_fungus), length_per_category)\n",
    "    poisonous_indices = np.random.choice(len(poisonous_fungus), length_per_category)\n",
    "    # Calculate the total sample count\n",
    "    sample_count = 2 * length_per_category\n",
    "    # Create a balanced dataset with the selected edible and poisonous fungi images\n",
    "    return get_dataset(\n",
    "        edible_fungus[edible_indices], \n",
    "        poisonous_fungus[poisonous_indices], \n",
    "        mode, \n",
    "        batch_size\n",
    "    ), sample_count\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edible_fungus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(poisonous_fungus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edible_fungus[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image size\n",
    "IMAGE_SIZE = 240\n",
    "\n",
    "# Set the name of the ResNet architecture to use\n",
    "handle_base = \"resnet_v2_101\"\n",
    "\n",
    "# Set the URL of the TensorFlow Hub module that provides the pre-trained ResNet feature extractor\n",
    "MODULE_HANDLE = \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature_vector/5\".format(handle_base)\n",
    "\n",
    "# Create a Keras layer using the pre-trained ResNet feature extractor\n",
    "feature_extractor = hub.KerasLayer(MODULE_HANDLE,\n",
    "                                    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "# Set the feature extractor to be non-trainable\n",
    "feature_extractor.trainable = False\n",
    "\n",
    "# Define batch size and number of batches\n",
    "batch_size = 32\n",
    "batch_count = min(len(edible_fungus), len(poisonous_fungus)) // batch_size\n",
    "\n",
    "edible_fungus = np.array(edible_fungus)\n",
    "poisonous_fungus = np.array(poisonous_fungus)\n",
    "\n",
    "# Get a balanced dataset\n",
    "train_dataset, train_steps = get_balanced_dataset(edible_fungus, poisonous_fungus, batch_count, batch_size, mode=\"train\")\n",
    "\n",
    "# Iterate over the batches in the dataset and extract features\n",
    "all_features = []\n",
    "for batch in train_dataset:\n",
    "    batch_images, _ = batch\n",
    "    batch_features = feature_extractor(batch_images)\n",
    "    all_features.append(batch_features)\n",
    "\n",
    "# Concatenate the output features into a single numpy array\n",
    "all_features = np.concatenate(all_features, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "img = Image.open(\"/content/th-2312586860.jpeg\")\n",
    "\n",
    "# Resize the image\n",
    "img = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "img_array = np.array(img)\n",
    "\n",
    "# Add an extra dimension to the array to represent the batch size of 1\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "\n",
    "# Pass the image through the pre-trained ResNet model to get the extracted features\n",
    "img_tensor = tf.convert_to_tensor(img_array, dtype=tf.float32)\n",
    "features = feature_extractor(img_tensor)\n",
    "\n",
    "\n",
    "# Define the classifier model\n",
    "classifier = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Make a prediction using the classifier model\n",
    "prediction = classifier(features).numpy()[0][0]\n",
    "\n",
    "# Print the prediction\n",
    "if prediction >= 0.5:\n",
    "    print(\"The image is predicted to be poisonous.\")\n",
    "else:\n",
    "    print(\"The image is predicted to be edible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_images, _ = next(iter(new_dataset))\n",
    "\n",
    "# predictions = model.predict(new_images)\n",
    "\n",
    "# # The predictions will be a numpy array with shape (batch_size, num_classes)\n",
    "# # Can use numpy functions like argmax or softmax to interpret the predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
